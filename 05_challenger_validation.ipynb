{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "574c57ea-5713-437d-a0ab-58d6a773af36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=https://raw.githubusercontent.com/aestaire/ml_workshop/refs/heads/main/files/images/hands-on.png>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4bb7d7a-075f-46c4-976b-a3c6b550355c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Validación del modelo Challenger\n",
    "\n",
    "Este notebook realiza tareas de validación en el modelo candidato __Challenger__.\n",
    "\n",
    "Pasa por algunos pasos para validar el modelo antes de etiquetarlo (asignándole el alias) como `Challenger`.\n",
    "\n",
    "Cuando las organizaciones comienzan a implementar procesos de MLOps, deberían considerar tener un \"humano en el circuito\" para realizar análisis visuales y validar los modelos antes de promoverlos. A medida que se familiarizan con el proceso, pueden considerar automatizar los pasos en un __Workflow__. El beneficio de la automatización es asegurar que estas comprobaciones de validación se realicen sistemáticamente antes de que los nuevos modelos se integren en los pipelines de inferencia o se desplieguen para el servicio en tiempo real. Por supuesto, las organizaciones pueden optar por mantener un \"humano en el circuito\" en cualquier parte del proceso y establecer el grado de automatización que se adapte a sus necesidades empresariales.\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/mlops/mlops-uc-end2end-4-v2.png?raw=true\" width=\"1200\">\n",
    "\n",
    "*Nota: en una configuración típica de MLOps, esto se ejecutaría como parte de un trabajo automatizado para validar un nuevo modelo. Ejecutaremos esta sencilla demostración como un notebook interactivo.*\n",
    "\n",
    "<!-- Recopilar datos de uso (vista). Elimínelo para desactivar la recopilación o desactive el rastreador durante la instalación. Consulte el README para más detalles.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-science&org_id=1444828305810485&notebook=%2F01-mlops-quickstart%2F04_challenger_validation&demo_name=mlops-end2end&event=VIEW&path=%2F_dbdemos%2Fdata-science%2Fmlops-end2end%2F01-mlops-quickstart%2F04_challenger_validation&version=1&user_hash=f7ea13a45c991650d8df810431c3e0e2b12887e9ed7e206ee8fb6209bdb2ae82\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eda3a04d-4ac3-4de4-b025-57c116400816",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Se ha creado un cluster para esta demostración\n",
    "Para ejecutar esta demostración, simplemente selecciona el cluster `dbdemos-mlops-end2end` en el menú desplegable ([abrir configuración del clúster](https://e2-demo-field-eng.cloud.databricks.com/#setting/clusters/1013-181446-g00pu2bj/configuration)). <br />\n",
    "*Nota: Si el cluster fue eliminado después de 30 días, puedes recrearlo con `dbdemos.create_cluster('mlops-end2end')` o reinstalar la demo: `dbdemos.install('mlops-end2end')`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "841144ab-1e3b-4ec1-8c36-2b3d1f982717",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Último ambiente testado:\n",
    "\n",
    "mlflow==3.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "5e2afbbd-270a-41cf-b26c-9b04c00f1f04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Comprobaciones Generales de Validación\n",
    "\n",
    "<!--img style=\"float: right\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/retail/resources/images/churn-mlflow-webhook-1.png\" width=600 -->\n",
    "\n",
    "En el contexto de MLOps, existen más pruebas que simplemente la precisión de un modelo. Para garantizar la estabilidad de nuestro sistema de ML y el cumplimiento de cualquier requisito normativo, someteremos cada modelo añadido al registro a una serie de comprobaciones de validación. Estas incluyen, pero no se limitan a:\n",
    "<br>\n",
    "* __Documentación del modelo__\n",
    "* __Inferencia sobre datos de producción__\n",
    "* __Pruebas Champion-Challenger para asegurar que los KPIs de negocio sean aceptables__\n",
    "\n",
    "En este notebook, exploramos algunos enfoques para realizar estas pruebas y cómo podemos añadir metadatos a nuestros modelos etiquetando si han pasado una prueba determinada.\n",
    "\n",
    "Esta parte suele ser específica para tu línea de negocio y requisitos de calidad.\n",
    "\n",
    "Para cada prueba, agregaremos información usando etiquetas para saber qué se ha validado en el modelo. También podemos añadir comentarios a un modelo si es necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d02c80b1-a4e8-42ef-bcd3-c1e032ec5e3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet mlflow --upgrade\n",
    "\n",
    "\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38852558-960a-4018-9914-818cc0e23813",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../_resources/00-setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94a20639-e47f-4e00-96cd-ff6c90cbda49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.store.artifact.models_artifact_repo import ModelsArtifactRepository\n",
    "\n",
    "\n",
    "requirements_path = ModelsArtifactRepository(f\"models:/{catalog}.{db}.mlops_churn@Challenger\").download_artifacts(artifact_path=\"requirements.txt\") # download model from remote registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52f52cd7-589c-46fc-930c-a61290f504d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet -r $requirements_path\n",
    "\n",
    "\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30604552-789b-435c-b5b8-8a4a4980e1f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../_resources/00-setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0359b32c-e9af-46f8-9ec6-bf41da6c0c4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Obtener información del modelo\n",
    "\n",
    "Obtendremos la información del modelo __Challenger__ desde Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2a80bc1-425b-4a6b-aa45-4ea31a83be1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We are interested in validating the Challenger model\n",
    "model_alias = \"Challenger\"\n",
    "model_name = f\"{catalog}.{db}.mlops_churn\"\n",
    "\n",
    "client = MlflowClient()\n",
    "model_details = client.get_model_version_by_alias(model_name, model_alias)\n",
    "model_version = int(model_details.version)\n",
    "\n",
    "print(f\"Validating {model_alias} model for {model_name} on model version {model_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57d56a9f-66b4-4ba7-bb11-957fe7a04a27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Comprobaciones del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdb0a129-dd10-4e6d-ae6a-8876ad79180a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Verificación de la descripción\n",
    "\n",
    "¿El científico de datos proporcionó una descripción del modelo que está siendo enviado?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5d5f5fd-26c9-4732-99fc-f76c63df400b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# If there's no description or an insufficient number of characters, tag accordingly\n",
    "if not model_details.description:\n",
    "  has_description = False\n",
    "  print(\"Please add model description\")\n",
    "elif not len(model_details.description) > 20:\n",
    "  has_description = False\n",
    "  print(\"Please add detailed model description (40 char min).\")\n",
    "else:\n",
    "  has_description = True\n",
    "\n",
    "print(f'Model {model_name} version {model_details.version} has description: {has_description}')\n",
    "client.set_model_version_tag(name=model_name, version=str(model_details.version), key=\"has_description\", value=has_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "702d7855-9121-46d6-856f-9c64106b4768",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Métrica de desempeño del modelo\n",
    "\n",
    "Queremos validar la métrica de desempeño del modelo. Normalmente, queremos comparar esta métrica obtenida para el modelo Challenger contra la del modelo Champion. Como aún no hemos registrado un modelo Champion, solo recuperaremos la métrica para el modelo Challenger sin hacer una comparación.\n",
    "\n",
    "El modelo registrado captura información sobre la ejecución del experimento MLflow, donde las métricas del modelo se registraron durante el entrenamiento. Esto le brinda trazabilidad desde el modelo implementado hasta las ejecuciones de entrenamiento iniciales.\n",
    "\n",
    "Aquí, usaremos el puntaje F1 para el conjunto de datos de prueba reservado durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e144b56-8afb-4240-8cac-b84dfb30f642",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_run_id = model_details.run_id\n",
    "f1_score = mlflow.get_run(model_run_id).data.metrics['val_f1_score']\n",
    "\n",
    "try:\n",
    "    #Compare the challenger f1 score to the existing champion if it exists\n",
    "    champion_model = client.get_model_version_by_alias(model_name, \"Champion\")\n",
    "    champion_f1 = mlflow.get_run(champion_model.run_id).data.metrics['val_f1_score']\n",
    "    print(f'Champion f1 score: {champion_f1}. Challenger f1 score: {f1_score}.')\n",
    "    metric_f1_passed = f1_score >= champion_f1\n",
    "except:\n",
    "    print(f\"No Champion found. Accept the model as it's the first one.\")\n",
    "    metric_f1_passed = True\n",
    "\n",
    "print(f'Model {model_name} version {model_details.version} metric_f1_passed: {metric_f1_passed}')\n",
    "# Tag that F1 metric check has passed\n",
    "client.set_model_version_tag(name=model_name, version=model_details.version, key=\"metric_f1_passed\", value=metric_f1_passed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2bdf925-3491-47e7-b966-62a9f920ae12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Métricas de referencia o de negocio en el conjunto de datos de evaluación\n",
    "\n",
    "Vamos a usar nuestro conjunto de datos de validación para verificar el impacto potencial del nuevo modelo.\n",
    "\n",
    "***Nota: Esto es solo para evaluar nuestros modelos, no debe confundirse con la prueba A/B**. La prueba A/B se realiza en línea, dividiendo el tráfico entre 2 modelos. Requiere un ciclo de retroalimentación para evaluar el efecto de la predicción (por ejemplo, después de una predicción, ¿el descuento que ofrecimos al cliente evitó el churn?). Cubriremos la prueba A/B en la parte avanzada.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ecdfee2-06e0-4c3f-9dca-6ce4686541e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "# Get the eval dataset:\n",
    "eval_df = spark.table('mlops_churn_training').filter(\"split='test'\")\n",
    "\n",
    "# Call the model with the given alias and return the prediction\n",
    "def predict_churn(df, model_alias):\n",
    "    model = mlflow.pyfunc.spark_udf(spark, model_uri=f\"models:/{catalog}.{db}.mlops_churn@{model_alias}\") #Use env_manager=\"virtualenv\" to recreate a venv with the same python version if needed\n",
    "    return df.withColumn('predictions', model(*model.metadata.get_input_schema().input_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e024d004-2a25-4619-8fc4-bd4b1dd1017d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "#Note: this is over-simplified and depends on your use-case, but the idea is to evaluate our model against business metrics\n",
    "cost_of_customer_churn = 2000 #in dollar\n",
    "cost_of_discount = 500 #in dollar\n",
    "\n",
    "cost_true_negative = 0 #did not churn, we did not give him the discount\n",
    "cost_false_negative = cost_of_customer_churn #did churn, we lost the customer\n",
    "cost_true_positive = cost_of_customer_churn -cost_of_discount #We avoided churn with the discount\n",
    "cost_false_positive = -cost_of_discount #doesn't churn, we gave the discount for free\n",
    "\n",
    "def get_model_value_in_dollar(model_alias):\n",
    "    # Convert preds_df to Pandas DataFrame\n",
    "    model_predictions = predict_churn(eval_df, model_alias).toPandas()\n",
    "    # Calculate the confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(model_predictions['churn'], model_predictions['predictions']).ravel()\n",
    "    return tn * cost_true_negative+ fp * cost_false_positive + fn * cost_false_negative + tp * cost_true_positive\n",
    "#add an exception to catch non-existing model champion yet\n",
    "is_champ_model_exist = True\n",
    "try:\n",
    "    client.get_model_version_by_alias(f\"{catalog}.{db}.mlops_churn\", \"Champion\")\n",
    "    print(\"Model already registered as Champion\")\n",
    "except Exception as error:\n",
    "    print(\"An error occurred:\", type(error).__name__, \"It means no champion model yet exist\")\n",
    "    is_champ_model_exist = False\n",
    "if is_champ_model_exist:\n",
    "    champion_potential_revenue_gain = get_model_value_in_dollar(\"Champion\")\n",
    "    challenger_potential_revenue_gain = get_model_value_in_dollar(\"Challenger\")\n",
    "\n",
    "try:\n",
    "    #Compare the challenger f1 score to the existing champion if it exists\n",
    "    champion_potential_revenue_gain = get_model_value_in_dollar(\"Champion\")\n",
    "except:\n",
    "    print(f\"No Champion found. Accept the model as it's the first one.\")\n",
    "    champion_potential_revenue_gain = 0\n",
    "    \n",
    "challenger_potential_revenue_gain = get_model_value_in_dollar(\"Challenger\")\n",
    "\n",
    "data = {'Model Alias': ['Challenger', 'Champion'],\n",
    "        'Potential Revenue Gain': [challenger_potential_revenue_gain, champion_potential_revenue_gain]}\n",
    "\n",
    "# Create a bar plot using plotly express\n",
    "px.bar(data, x='Model Alias', y='Potential Revenue Gain', color='Model Alias',\n",
    "    labels={'Potential Revenue Gain': 'Revenue Impacted'},\n",
    "    title='Business Metrics - Revenue Impacted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "091baf36-f35b-464d-a535-d35589cbf4f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Resultados de validación\n",
    "\n",
    "¡Eso es todo! Hemos demostrado algunas comprobaciones simples en el modelo. Veamos los resultados de la validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c6c5022-f5e2-433d-99b5-76e6749b0f63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results = client.get_model_version(model_name, model_version)\n",
    "results.tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "508b66d9-af08-4579-b447-e3e2b84b12d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Promoviendo el Challenger a Champion\n",
    "\n",
    "Cuando estemos satisfechos con los resultados del modelo __Challenger__, podemos promoverlo a Champion. Esto se hace estableciendo su alias como `@Champion`. Las canalizaciones de inferencia que cargan el modelo usando el alias `@Champion` cargarán entonces este nuevo modelo. El alias en el modelo Champion anterior, si existe, se eliminará automáticamente. El modelo mantiene su alias `@Challenger` hasta que se implemente un nuevo modelo Challenger con el alias para reemplazarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e81b0b5b-2ed3-4766-bc17-ff1020c49e12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if results.tags[\"has_description\"] == \"True\" and results.tags[\"metric_f1_passed\"] == \"True\":\n",
    "  print('register model as Champion!')\n",
    "  client.set_registered_model_alias(\n",
    "    name=model_name,\n",
    "    alias=\"Champion\",\n",
    "    version=model_version\n",
    "  )\n",
    "else:\n",
    "  raise Exception(\"Model not ready for promotion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd8bd97a-9526-4d99-be73-af5ff47d456b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ¡Felicidades! Nuestro modelo ha sido validado y promovido correctamente\n",
    "\n",
    "Ahora tenemos la certeza de que nuestro modelo está listo para ser utilizado en canalizaciones de inferencia y endpoints de servicio en tiempo real, ya que cumple con nuestros estándares de validación.\n",
    "\n",
    "Siguiente: [Ejecutar inferencia por lotes desde nuestro nuevo modelo Champion promovido]($./05_batch_inference)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "05_challenger_validation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
