{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da11407a-8fc6-432a-be45-0b154eab4b7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(\"reset_all_data\", \"false\", [\"true\", \"false\"], \"Reset all data\")\n",
    "dbutils.widgets.dropdown(\"gen_synthetic_data\", \"false\", [\"true\", \"false\"], \"Generate Synthetic data for Drift Detection\")\n",
    "dbutils.widgets.dropdown(\"adv_mlops\", \"false\", [\"true\", \"false\"], \"Setup for advanced MLOps demo\")\n",
    "dbutils.widgets.dropdown(\"setup_inference_data\", \"false\", [\"true\", \"false\"], \"Setup inference data for quickstart\")\n",
    "dbutils.widgets.dropdown(\"setup_adv_inference_data\", \"false\", [\"true\", \"false\"], \"Setup inference data for advanced demo\")\n",
    "reset_all_data = dbutils.widgets.get(\"reset_all_data\") == \"true\"\n",
    "setup_inference_data = dbutils.widgets.get(\"setup_inference_data\") == \"true\"\n",
    "setup_adv_inference_data = dbutils.widgets.get(\"setup_adv_inference_data\") == \"true\"\n",
    "generate_synthetic_data = dbutils.widgets.get(\"gen_synthetic_data\") == \"true\"\n",
    "is_advanced_mlops_demo = dbutils.widgets.get(\"adv_mlops\") == \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7ecade9-3fc4-472e-a225-353481c5c84f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "current_user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
    "reformat_current_user = current_user.split(\"@\")[0].lower().replace(\".\", \"_\")\n",
    "\n",
    "catalog = \"workshop_databricks\"\n",
    "dbName = db = \"dbdemos_mlops\"\n",
    "online_store_name = \"shared_online_store\" # \"fe_shared_demo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba29d433-d8ab-4b65-ac84-cdc896ddaf5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./00-global-setup-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef45f5d0-b9b9-4cc3-a3d0-fd3bb1214286",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "#remove warnings for nicer display\n",
    "import warnings\n",
    "import logging\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger(\"mlflow\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2af95878-5425-43dc-afa8-9aa107e566d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DBDemos.setup_schema(catalog, db, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bbf4682-a963-4643-a07b-414a25749e22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set UC Model Registry as default\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "xp_name = \"dbdemos_mlops_churn_demo_experiment\"\n",
    "xp_path = f\"/Users/{current_user}/dbdemos_mlops\"\n",
    "if is_advanced_mlops_demo:\n",
    "  # Set Experiment name as default\n",
    "  xp_name = \"dbdemos_advanced_mlops_churn_demo_experiment\"\n",
    "try:\n",
    "  from databricks.sdk import WorkspaceClient\n",
    "  w = WorkspaceClient()\n",
    "  r = w.workspace.mkdirs(path=xp_path)\n",
    "except Exception as e:\n",
    "  print(f\"ERROR: couldn't create a folder for the experiment under {xp_path} - please create the folder manually or  skip this init (used for job only: {e})\")\n",
    "  raise e\n",
    "\n",
    "client = MlflowClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34b0d3a6-e90f-443d-a91e-eff1ed33d6b6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Raw/Bronze customer data from IBM Telco public dataset and sanitize column name"
    }
   },
   "outputs": [],
   "source": [
    "# Default for quickstart\n",
    "bronze_table_name = \"mlops_churn_bronze_customers\"\n",
    "\n",
    "# Bronze table name for advanced\n",
    "if is_advanced_mlops_demo:\n",
    "  bronze_table_name = \"advanced_churn_bronze_customers\"\n",
    "\n",
    "if reset_all_data or not spark.catalog.tableExists(bronze_table_name):\n",
    "  import requests\n",
    "  from io import StringIO\n",
    "  #Dataset under apache license: https://github.com/IBM/telco-customer-churn-on-icp4d/blob/master/LICENSE\n",
    "  csv = requests.get(\"https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv\").text\n",
    "  df = pd.read_csv(StringIO(csv), sep=\",\")\n",
    "  def cleanup_column(pdf):\n",
    "    # Clean up column names\n",
    "    pdf.columns = [re.sub(r'(?<!^)(?=[A-Z])', '_', name).lower().replace(\"__\", \"_\") for name in pdf.columns]\n",
    "    pdf.columns = [re.sub(r'[\\(\\)]', '', name).lower() for name in pdf.columns]\n",
    "    pdf.columns = [re.sub(r'[ -]', '_', name).lower() for name in pdf.columns]\n",
    "    return pdf.rename(columns = {'streaming_t_v': 'streaming_tv', 'customer_i_d': 'customer_id'})\n",
    "  \n",
    "  if is_advanced_mlops_demo:\n",
    "    from databricks.sdk import WorkspaceClient\n",
    "    w = WorkspaceClient()\n",
    "    try:\n",
    "      print(f\"Deleting existing monitors for {catalog}.{db}.advanced_churn_inference_table\")\n",
    "      w.quality_monitors.delete(table_name=f\"{catalog}.{db}.advanced_churn_inference_table\")\n",
    "    except Exception as error:\n",
    "      print(f\"Error deleting monitor: {type(error).__name__}\")\n",
    "    experiment_details = client.get_experiment_by_name(f\"{xp_path}/{xp_name}\")\n",
    "    if experiment_details:\n",
    "      print(f' Deleting experiment: {experiment_details.experiment_id}')\n",
    "      client.delete_experiment(f'{experiment_details.experiment_id}')\n",
    "  \n",
    "  df = cleanup_column(df)\n",
    "  print(f\"creating `{bronze_table_name}` raw table\")\n",
    "  spark.createDataFrame(df).write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(bronze_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f2b56b9-e21f-4c14-8202-9cc0528433af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def delete_feature_store_table(catalog, db, feature_table_name):\n",
    "  from databricks.feature_engineering import FeatureEngineeringClient\n",
    "  fe = FeatureEngineeringClient()\n",
    "  try:\n",
    "    # Drop existing table from Feature Store\n",
    "    fe.drop_table(name=f\"{catalog}.{db}.{feature_table_name}\")\n",
    "    # Delete underyling delta tables\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {catalog}.{db}.{feature_table_name}\")\n",
    "    print(f\"Dropping Feature Table {catalog}.{db}.{feature_table_name}\")\n",
    "  except ValueError as ve:\n",
    "    print(f\"Feature Table {catalog}.{db}.{feature_table_name} doesn't exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e41b50da-c5c0-4526-ab8f-cacc1b823201",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This setup is used in the quickstart demo only\n",
    "quickstart_training_table_name = \"mlops_churn_training\"\n",
    "quickstart_unlabelled_table_name = \"mlops_churn_inference\"\n",
    "\n",
    "if setup_inference_data:\n",
    "  # Check that the training table exists first, as we'll be creating a copy of it\n",
    "  if spark.catalog.tableExists(f\"{catalog}.{db}.{quickstart_training_table_name}\"):\n",
    "    # This should only be called from the quickstart challenger validation or batch inference notebooks\n",
    "    if not spark.catalog.tableExists(f\"{catalog}.{db}.{quickstart_unlabelled_table_name}\"):\n",
    "      print(\"Creating unlabelled data table for performing inference...\")\n",
    "      # Drop the label column for inference\n",
    "      spark.read.table(quickstart_training_table_name).drop(\"churn\").write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(quickstart_unlabelled_table_name)\n",
    "  else:\n",
    "    print(\"Training table doesn't exist, please run the notebook '01_feature_engineering'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0aa62150-71f1-4504-a903-2d2f4a01b142",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This setup is used in the advanced demo only\n",
    "#advanced_label_table_name = \"churn_label_table\"\n",
    "#advanced_unlabelled_table_name = \"mlops_churn_advanced_cust_ids\"\n",
    "\n",
    "if setup_adv_inference_data:\n",
    "  # Check that the label table exists first, as we'll be creating a copy of it\n",
    "  if spark.catalog.tableExists(f\"advanced_churn_label_table\"):\n",
    "    # This should only be called from the advanced batch inference notebook\n",
    "    # if not spark.catalog.tableExists(f\"advanced_churn_cust_ids\"):\n",
    "    print(\"Creating table with customer records for inference...\")\n",
    "    # Drop the label column for inference\n",
    "    spark.read.table(\"advanced_churn_label_table\").drop(\"churn\").write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"advanced_churn_cust_ids\")\n",
    "  else:\n",
    "    print(\"Label table `advanced_churn_label_table` doesn't exist, please run the notebook '01_feature_engineering'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea57b3e6-77ac-4850-a8e0-f5bd71cd0cd2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate inference synthetic data"
    }
   },
   "outputs": [],
   "source": [
    "def generate_synthetic(inference_table):\n",
    "  import dbldatagen as dg\n",
    "  import pyspark.sql.types\n",
    "  import pyspark.sql.functions as F\n",
    "  from datetime import datetime, timedelta\n",
    "\n",
    "  model_version = client.get_model_version_by_alias(name=model_name, alias=\"Champion\").version\n",
    "  n_days_to_back_fill = 30 # Change this\n",
    "\n",
    "  # Column definitions are based on original dataset schema\n",
    "  generation_spec = (\n",
    "    dg.DataGenerator(sparkSession=spark, \n",
    "                    name='synthetic_data', \n",
    "                    rows=7000*n_days_to_back_fill,\n",
    "                    random=True,\n",
    "                    )\n",
    "    .withColumn('customer_id', 'string', template=r'dddd-AAAA')\n",
    "    .withColumn('transaction_ts', 'timestamp', begin=(datetime.now() + timedelta(days=-n_days_to_back_fill)), end=datetime.now(), interval=\"1 hour\")\n",
    "    .withColumn('gender', 'string', values=['Female', 'Male'], random=True, weights=[0.5, 0.5])\n",
    "    .withColumn('senior_citizen', 'string', values=['No', 'Yes'], random=True, weights=[0.85, 0.15])\n",
    "    .withColumn('partner', 'string', values=['No', 'Yes'], random=True, weights=[0.5, 0.5])\n",
    "    .withColumn('dependents', 'string', values=['No', 'Yes'], random=True, weights=[0.7, 0.3])\n",
    "    .withColumn('tenure', 'double', minValue=0.0, maxValue=72.0, step=1.0)\n",
    "    .withColumn('phone_service', values=['No', 'Yes'], random=True, weights=[0.9, 0.1])\n",
    "    .withColumn('multiple_lines', 'string', values=['No', 'Yes'], random=True, weights=[0.5, 0.5])\n",
    "    .withColumn('internet_service', 'string', values=['Fiber optic', 'DSL', 'No'], random=True, weights=[0.5, 0.3, 0.2])\n",
    "    .withColumn('online_security', 'string', values=['No', 'Yes'], random=True, weights=[0.5, 0.5])\n",
    "    .withColumn('online_backup', 'string', values=['No', 'Yes'], random=True, weights=[0.5, 0.5])\n",
    "    .withColumn('device_protection', 'string', values=['No', 'Yes'], random=True, weights=[0.5, 0.5])\n",
    "    .withColumn('tech_support', 'string', values=['No', 'Yes'], random=True, weights=[0.5, 0.5])\n",
    "    .withColumn('streaming_tv', 'string', values=['No', 'Yes', 'No internet service'], random=True, weights=[0.4, 0.4, 0.2])\n",
    "    .withColumn('streaming_movies', 'string', values=['No', 'Yes', 'No internet service'], random=True, weights=[0.4, 0.4, 0.2])\n",
    "    .withColumn('contract', 'string', values=['Month-to-month', 'One year','Two year'], random=True, weights=[0.5, 0.25, 0.25])\n",
    "    .withColumn('paperless_billing', 'string', values=['No', 'Yes'], random=True, weights=[0.6, 0.4])\n",
    "    .withColumn('payment_method', 'string', values=['Credit card (automatic)', 'Mailed check', 'Bank transfer (automatic)', 'Electronic check'], weights=[0.2, 0.2, 0.2, 0.4])\n",
    "    .withColumn('monthly_charges', 'double', minValue=18.0, maxValue=118.0, step=0.5)\n",
    "    .withColumn('total_charges', 'double', minValue=0.0, maxValue=8684.0, step=20)\n",
    "    .withColumn('num_optional_services', 'double', minValue=0.0, maxValue=6.0, step=1)\n",
    "    .withColumn('avg_price_increase', 'float', minValue=-19.0, maxValue=130.0, step=20)\n",
    "    .withColumn('churn', 'string', values=['No', 'Yes'], random=True, weights=[0.2, 0.8], percentNulls=0.8)\n",
    "    .withColumn('inference_timestamp', 'timestamp', begin=(datetime.now() + timedelta(days=-n_days_to_back_fill)), end=(datetime.now()), interval=\"1 hour\")\n",
    "    .withColumn('prediction', 'string', values=['No', 'Yes'], random=True, weights=[0.6, 0.4])\n",
    "    )\n",
    "\n",
    "  # Generate Synthetic Data\n",
    "  df_synthetic_data = generation_spec.build()\n",
    "\n",
    "  ## Append relevant/monitoring columns\n",
    "  preds_df = df_synthetic_data \\\n",
    "    .withColumn('model_name', F.lit(model_name)) \\\n",
    "    .withColumn('model_version', F.lit(model_version)) \\\n",
    "    # .withColumn('inference_timestamp', F.lit(datetime.now())) # + timedelta(days=1))) \n",
    "\n",
    "  preds_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(f\"{catalog}.{db}.{inference_table_name}\")\n",
    "\n",
    "if is_advanced_mlops_demo:\n",
    "  model_name = f\"{catalog}.{db}.advanced_mlops_churn\"\n",
    "  inference_table_name = \"advanced_churn_inference_table\"\n",
    "  if generate_synthetic_data:\n",
    "    generate_synthetic(inference_table=inference_table_name)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00-setup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
