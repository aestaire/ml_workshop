{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4f9cdbe-162f-4024-be60-bf8005db4acf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=https://raw.githubusercontent.com/aestaire/ml_workshop/refs/heads/main/files/images/hands-on.png>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1250481a-e295-4095-8099-815f514f8f64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Churn Prediction Feature Engineering\n",
    "Nuestro primer paso es analizar los datos y construir las features que usaremos para entrenar nuestro modelo. Veamos cómo se puede hacer.\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/mlops/mlops-uc-end2end-1-v2.png?raw=true\" width=\"1200\">\n",
    "\n",
    "<!-- Recopilar datos de uso (vista). Elimínelo para deshabilitar la recopilación o desactive el rastreador durante la instalación. Consulte el README para más detalles.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-science&org_id=1444828305810485&notebook=%2F01-mlops-quickstart%2F01_feature_engineering&demo_name=mlops-end2end&event=VIEW&path=%2F_dbdemos%2Fdata-science%2Fmlops-end2end%2F01-mlops-quickstart%2F01_feature_engineering&version=1&user_hash=f7ea13a45c991650d8df810431c3e0e2b12887e9ed7e206ee8fb6209bdb2ae82\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05f24747-89bf-447f-bb2f-229f6a9bdc86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Se ha creado un cluster para esta demostración\n",
    "Para ejecutar esta demostración, simplemente selecciona el clúster `dbdemos-mlops-end2end` en el menú desplegable ([abrir configuración del clúster](https://e2-demo-field-eng.cloud.databricks.com/#setting/clusters/1013-181446-g00pu2bj/configuration)). <br />\n",
    "*Nota: Si el clúster fue eliminado después de 30 días, puedes volver a crearlo con `dbdemos.create_cluster('mlops-end2end')` o reinstalar la demo: `dbdemos.install('mlops-end2end')`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6085dc7b-5acf-479d-afa6-a91745fcfab2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Último environment testado:\n",
    "\n",
    "mlflow==3.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f738ac40-5e27-483a-94e3-5e8e04c18293",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet mlflow --upgrade\n",
    "\n",
    "\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e996963-5eec-40ee-bd6c-c2a7a30dcf48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../_resources/00-setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e17d5e1-9342-46dd-8c08-e6087b42a21a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Análisis exploratorio de datos\n",
    "Para familiarizarse con los datos, identificar qué necesita limpieza, preprocesamiento, etc.\n",
    "- **Utiliza las herramientas nativas de visualización de Databricks**\n",
    "  - Después de ejecutar una consulta SQL en una celda del notebook, usa la pestaña `+` para agregar gráficos y visualizar los resultados.\n",
    "- Trae tu propia librería de visualización preferida (por ejemplo, seaborn, plotly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95942cef-8ee1-4746-83fb-4a6293ff51bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM mlops_churn_bronze_customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d612cefe-333c-4c65-8d9b-c3a86be3f3e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "telco_df = spark.read.table(\"mlops_churn_bronze_customers\").pandas_api()\n",
    "telco_df[\"internet_service\"].value_counts().plot.pie()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "465b0535-727c-4b03-9e29-921516a9d822",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read in Bronze Delta table using Spark"
    }
   },
   "outputs": [],
   "source": [
    "# Read into Spark\n",
    "telcoDF = spark.read.table(\"mlops_churn_bronze_customers\")\n",
    "display(telcoDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c6c6152-a8e0-46a4-85bf-f1de19d7ae06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Definir lógica de limpieza y creación de features\n",
    "\n",
    "Vamos a definir una función para limpiar los datos e implementar la lógica de creación de features. Para ello vamos a:\n",
    "\n",
    "1. Calcular el número de servicios opcionales\n",
    "2. Proporcionar etiquetas significativas\n",
    "3. Imputar valores nulos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32dc3ccb-e834-4eda-8870-03fa0e5ef5cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Usando la API de Pandas en Spark\n",
    "\n",
    "Como nuestro equipo de científicos de datos está familiarizado con Pandas, utilizaremos la [API de pandas en spark](https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/index.html) para escalar el código de `pandas`. Las instrucciones de Pandas se convertirán en el motor de spark internamente y se distribuirán a escala.\n",
    "\n",
    "*Nota: La API de Pandas en Spark antes se llamaba Koalas. A partir de `spark 3.2`, Koalas está incorporado y podemos obtener un DataFrame de Pandas usando `pandas_api()` [Detalles](https://spark.apache.org/docs/latest/api/python/migration_guide/koalas_to_pyspark.html).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72a70c4b-9295-4bcc-aaed-2f58c4af8c90",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define featurization function"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "\n",
    "def clean_churn_features(dataDF: DataFrame) -> DataFrame:\n",
    "  \"\"\"\n",
    "  Simple cleaning function leveraging pandas API\n",
    "  \"\"\"\n",
    "\n",
    "  # Convert to pandas on spark dataframe\n",
    "  data_psdf = dataDF.pandas_api()\n",
    "  # Convert some columns\n",
    "  data_psdf = data_psdf.astype({\"senior_citizen\": \"string\"})\n",
    "  data_psdf[\"senior_citizen\"] = data_psdf[\"senior_citizen\"].map({\"1\" : \"Yes\", \"0\" : \"No\"})\n",
    "\n",
    "  data_psdf[\"total_charges\"] = data_psdf[\"total_charges\"].apply(lambda x: float(x) if x.strip() else 0)\n",
    "\n",
    "\n",
    "  # Fill some missing numerical values with 0\n",
    "  data_psdf = data_psdf.fillna({\"tenure\": 0.0})\n",
    "  data_psdf = data_psdf.fillna({\"monthly_charges\": 0.0})\n",
    "  data_psdf = data_psdf.fillna({\"total_charges\": 0.0})\n",
    "\n",
    "  def sum_optional_services(df):\n",
    "      \"\"\"Count number of optional services enabled, like streaming TV\"\"\"\n",
    "      cols = [\"online_security\", \"online_backup\", \"device_protection\", \"tech_support\",\n",
    "              \"streaming_tv\", \"streaming_movies\"]\n",
    "      return sum(map(lambda c: (df[c] == \"Yes\"), cols))\n",
    "\n",
    "  data_psdf[\"num_optional_services\"] = sum_optional_services(data_psdf)\n",
    "\n",
    "  # Return the cleaned Spark dataframe\n",
    "  return data_psdf.to_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "13bdca83-654d-431d-b190-5b6502e1d561",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Calcular features y guardar tabla con features y etiquetas\n",
    "\n",
    "Una vez que nuestras features estén listas, las guardaremos junto con las etiquetas como una tabla Delta Lake. Luego, esta tabla podrá ser recuperada para el entrenamiento del modelo.\n",
    "\n",
    "En esta demostración rápida, veremos cómo entrenar un modelo usando este conjunto de datos etiquetado guardado como una tabla Delta Lake y cómo capturar la trazabilidad entre la tabla y el modelo. La trazabilidad del modelo aporta control y gobernanza a nuestro despliegue, permitiéndonos saber qué modelo depende de qué conjunto de tablas de features.\n",
    "\n",
    "Databricks tiene una capacidad de Feature Store (almacén de features) totalmente integrada en la plataforma. Cualquier tabla Delta Lake con una clave primaria puede usarse como tabla de features para el entrenamiento de modelos y para el servicio batch y en línea. Veremos un ejemplo de cómo usar el Feature Store para realizar búsquedas de features en una demostración más avanzada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab75f34e-bd6e-4a10-8c56-e179b41b0612",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Compute Churn Features and append a timestamp"
    }
   },
   "outputs": [],
   "source": [
    "churn_features = clean_churn_features(telcoDF)\n",
    "display(churn_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b35061cb-7e6b-471f-9d94-aa7961441e52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Escribir la tabla para entrenamiento\n",
    "\n",
    "Escribe los datos etiquetados que tienen las features preparadas y las etiquetas como una tabla Delta. Luego usaremos esta tabla para entrenar el modelo para predecir la deserción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8240908c-1efe-47cb-bfdb-cf8729473a47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Specify train-test split\n",
    "train_ratio, test_ratio = 0.8, 0.2\n",
    "churn_features = (churn_features.withColumn(\"random\", F.rand(seed=42))\n",
    "                                .withColumn(\"split\",\n",
    "                                            F.when(F.col(\"random\") < train_ratio, \"train\")\n",
    "                                            .otherwise(\"test\"))\n",
    "                                .drop(\"random\"))\n",
    "\n",
    "# Write table for training\n",
    "(churn_features.write.mode(\"overwrite\")\n",
    "               .option(\"overwriteSchema\", \"true\")\n",
    "               .saveAsTable(\"mlops_churn_training\"))\n",
    "\n",
    "# Add comment to the table\n",
    "spark.sql(f\"\"\"COMMENT ON TABLE {catalog}.{db}.mlops_churn_training IS \\'The features in this table are derived from the mlops_churn_bronze_customers table in the lakehouse. \n",
    "              We created service features and cleaned up their names.  No aggregations were performed.'\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6455b00b-aaf5-47d7-87db-90432831ce9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "¡Eso es todo! Las features etiquetadas ya están listas para ser utilizadas en el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f287d3a0-e317-4c15-92aa-b86e2c4a3cff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Entrenar un modelo base\n",
    "\n",
    "Próximo paso: [Entrenar un modelo lightGBM]($./02_train_lightGBM)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3873581079779473,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "02_feature_engineering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
